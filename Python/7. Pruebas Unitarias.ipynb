{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c5b5f9-9922-4c49-a62a-fed6d60bfc1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 7. Pruebas Unitarias\n",
    "\n",
    "- Las pruebas unitarias se utilizan para verificar la funcionalidad de métodos o funciones.\n",
    "- Son pruebas automatizadas que se deben escribir en código para probar funciones.\n",
    "- Se deben nombrar adecuadamente las funciones de prueba para identificar que se esta probando.\n",
    "- Se deben probar casos edge no solo los normales y obvios.\n",
    "- Se deben mantener las pruebas independientes, cada prueba es independiente de las demás.\n",
    "- La herramienta mas utilizada es pytest.\n",
    "\n",
    "## pytest\n",
    "- Es un framework de pruebas unitarias para Python.\n",
    "- Proporciona una sintaxis sencilla y flexible para escribir pruebas.\n",
    "- Permite agrupar pruebas en módulos y clases.\n",
    "- Soporta fixtures para configurar el estado antes de ejecutar las pruebas.\n",
    "\n",
    "- `pytest <nombre_archivo>.py`: Ejecuta las pruebas definidas en el archivo especificado.\n",
    "- `pytest <directorio_test>/`: Ejecuta todas las pruebas en el directorio especificado y sus subdirectorios.\n",
    "- `pytest <nombre_archivo>.py -k <expresión>`: Ejecuta solo las pruebas cuyos nombres coincidan con la expresión dada.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "549e40cd-6a58-40e0-9390-e16d32af11c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-06-23T23:46:22.679769Z",
     "start_time": "2025-06-23T23:46:22.629504Z"
    }
   },
   "source": [
    "import pytest\n",
    "\n",
    "def suma(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Se define una función de prueba que comienza con \"test_\"\n",
    "def test_suma():\n",
    "    assert suma(3, 4) == 7\n",
    "    assert suma(-1, 1) == 0\n",
    "    assert suma(-1, -1) == -2\n",
    "    isinstance(suma(1, 1), int)\n",
    "\n",
    "# Para ejecutar las pruebas unitarias se ejecuta el comando pytest <nombre_archivo>.py"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "e3a12d43-e9b8-4d78-bc02-4ab545ec883a",
   "metadata": {},
   "source": [
    "### Marcadores\n",
    "\n",
    "- Los marcadores son etiquetas que se pueden aplicar a las pruebas para agruparlas o filtrarlas.\n",
    "- Permiten ejecutar un subconjunto de pruebas basadas en ciertas condiciones.\n",
    "- Se definen usando el decorador @pytest.mark.<nombre_marcador>\n",
    "- @pytest.mark.skip: Se usa para saltar una prueba indefinidamente.\n",
    "- @pytest.mark.skipif(condición, reason=\"Razón para saltar\"): Se usa para saltar una prueba si se cumple una condición.\n",
    "- @pytest.mark.xfail: Se usa para marcar una prueba que se espera que falle, no se considera un error si falla.\n",
    "\n",
    "## Fixture\n",
    "\n",
    "- Permiten configurar el estado antes de ejecutar las pruebas. Se usan para preparar datos que serán usados en las pruebas.\n",
    "- Se definen usando el decorador @pytest.fixture\n",
    "- Las funciones de prueba pueden declarar las fixtures que necesitan como argumentos, pytest los inyecta en ejecución.\n",
    "- Los fixture pueden tener varios alcances:\n",
    "    * Function: Se crea una nueva instancia del fixture para cada función de prueba.\n",
    "    * Class: Se crea una nueva instancia del fixture para cada función de clase de prueba.\n",
    "    * Module y Session.\n",
    "- Los fixtures puede incluir un codigo de limpieza de datos que se ejecuta cuando la prueba finaliza, se utiliza la palabra clave yeld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94673f7-4ec1-477e-9153-300f03cbfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def datos_de_prueba():\n",
    "\n",
    "    # Configuración antes de la prueba\n",
    "    datos = {\"usuario\": \"jdoe\", \"password\": \"1234\"}\n",
    "\n",
    "    yield datos\n",
    "\n",
    "    # Limpieza después de la prueba\n",
    "    datos.clear()\n",
    "\n",
    "#Recibe como parametro el fixture.\n",
    "def test_login(datos_de_prueba):\n",
    "    usuario = datos_de_prueba[\"usuario\"]\n",
    "    password = datos_de_prueba[\"password\"]\n",
    "    assert usuario == \"jdoe\"\n",
    "    assert password == \"1234\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5986a3d8-8e2a-464f-9d3e-47d7f2ab14be",
   "metadata": {},
   "source": [
    "Para ejecutar las pruebas unitarias se debe ejecutar por consola el comando pytest, sus opciones son:\n",
    "\n",
    "Opción | Descripción | Ejemplo\n",
    "-------|-------------|--------\n",
    "-v, --verbose | Muestra más información durante la ejecución de las pruebas. | pytest -v\n",
    "-q, --quiet | Reduce la cantidad de salida de las pruebas. | pytest -q\n",
    "-x, --exitfirst | Detiene la ejecución de pruebas después del primer fallo. | pytest -x\n",
    "--maxfail=NUM | Detiene la ejecución después de que fallan NUM pruebas. | pytest --maxfail=2\n",
    "-k EXPRESSION | Ejecuta solo las pruebas cuyos nombres coincidan con la EXPRESSION. | pytest -k \"test_funcion\"\n",
    "-m MARKEXPR | Ejecuta solo las pruebas etiquetadas con MARKEXPR. | pytest -m \"marcador\"\n",
    "--durations=N | Muestra los N tests más lentos al final de la ejecución. | pytest --durations=3\n",
    "--tb=STYLE | Cambia el estilo del traceback, opciones: auto/long/short/line/native/no. | pytest --tb=short\n",
    "--lf, --last-failed | Ejecuta solo las pruebas que fallaron la última vez. | pytest --lf\n",
    "--ff, --failed-first | Ejecuta primero las pruebas que fallaron la última vez. | pytest --ff\n",
    "--disable-warnings | Desactiva la salida de advertencias. | pytest --disable-warnings\n",
    "--html=report.html | Genera un reporte HTML. | pytest --html=report.html\n",
    "--junitxml=report.xml | Genera un reporte en formato JUnit XML.\t| pytest --junitxml=report.xml\n",
    "--cov=PAQUETE | Mide la cobertura de código del PAQUETE especificado. | pytest --cov=mi_paquete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
